[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Meng’s Notes",
    "section": "",
    "text": "Preface\nIf you are reading this, you may be interesed in seeing what is “Alex’s Notes”.\nThese notes are just things that I am documenting, that I wish could become a useful resource for my future students, either when I TA or become a professor.\nHere’s how to learn anything:\n\nWrite it out! (Document, Code along)\nEXPERIMENT and explore\nVisualize things you don’t understand\nAsk Questions\nAnswer Exercise and Problems (stretch your knowledge)\nShare with like-minded individuals",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "courses/dls.html",
    "href": "courses/dls.html",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "1 ML Basics\nIn this set of notes, we will create a minimal version of PyTorch / Tensorflow from scratch, of what I call “PyStickOnFire”.\nThe (Supervised) Machine learning idea: we take a bunch of labeled data, feed them to a machine learning algorithm, and it outputs a “program” that solves the task.\nflowchart LR\n  A[Training Data Set X] --&gt;B[Machine Learning Algorithm]\n  B --&gt; C(Model h)\nIn this set of notes, we will focus on what the machine learning algorithm box contains. In general, it consists of three things:\nAll alogrithms in machine learning fit in this structure. Let’s look at softmax regression to illustrate these three basic components.",
    "crumbs": [
      "MACHINE LEARNING",
      "Deep Learning Systems"
    ]
  },
  {
    "objectID": "courses/dls.html#ml-basics",
    "href": "courses/dls.html#ml-basics",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "The hypothesis class (the structure of \\(h\\) in terms of a set of parameters)\nThe loss function (specifies how good a given hypothesis is)\nAn optimization method (the way to minimize the loss function)\n\n\n\nMulti-class Classification (Softmax Regression)\nConsider a k-class classification setting, where we have\n\ntraining data: \\[x^{(i)} \\in \\mathbb{R}^{n}, y^{(i)} \\in \\{1, \\dots, k \\} \\text{ for } i = 1, \\dots, m\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\n\nwhere the training data are vectors that looks like \\[X = \\{ \\begin{bmatrix} x^{(1)}_1 \\\\ x^{(1)}_2 \\\\ \\vdots \\\\ x^{(1)}_n \\end{bmatrix} , \\dots, \\begin{bmatrix} x^{(m)}_1 \\\\ x^{(m)}_2 \\\\ \\vdots \\\\ x^{(m)}_n \\end{bmatrix} \\}\\] and the labels are just a set of scalars of size \\(k\\).\n\n1st Element: The Hypothesis Function\nThe hypothesis function is a mapping from one input to one output. (Duhh… just like every other function there is). \\[h: \\mathbb{R^n} \\rightarrow \\mathbb{R^k}\\] \\[h(x) = \\begin{bmatrix} h_1(x) \\\\ h_2(x) \\\\ \\vdots \\\\ h_k(x)\\end{bmatrix}\\]\nSo what really is \\(h_i(x)\\)? It is the hypothesis, the “belief”, the probability of how likely \\(x\\) maps to class \\(i\\).\nA linear hypothesis function uses matrix multiplication, or some other linear way, for this transformation:\n\\[h_\\theta(x) = \\theta^T x\\]\nfor parameters \\(\\theta \\in \\mathbb{R^{n \\times k}}\\) (\\(n\\) rows and \\(k\\) columns, so transpose becomes \\(k \\times n\\), and \\(x \\in \\mathbb{R^{n \\times 1}}\\), so multiplication will work). Now we say \\(h_\\theta\\) because \\(\\theta\\) is the parameters.\nNotice how so far we only have one input and one output, \\(h\\) is only working on one instance of the training set. However, in order to implement these operations efficiently in the future, we shall use the matrix batch notation.\n\\[X \\in \\mathbb{R^{m \\times n}} = \\begin{bmatrix} -x^{(1)^{T}}- \\\\ -x^{(2)^{T}}- \\\\ \\vdots \\\\ -x^{(m)^{T}}- \\end{bmatrix}\\]\n\\[y \\in \\{ 1, \\dots, k \\}^m = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}  \\end{bmatrix}\\]\n\\[h_\\theta(X) = \\begin{bmatrix} -h_\\theta(x^{(1)})^{T}- \\\\ -h_\\theta(x^{(2)})^{T}- \\\\ \\vdots \\\\ -h_\\theta(x^{(m)})^{T}- \\end{bmatrix} = \\begin{bmatrix} -x^{(1)^{T}} \\theta- \\\\ -x^{(2)^{T}} \\theta- \\\\ \\vdots \\\\ -x^{(m)^{T}} \\theta- \\end{bmatrix} = X\\theta\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\nEach row is for a data point, the first example is in first row (originally a column vector, now we transposed it to row vector), and the second example is in second row, and so on… Note that this is not merely a notation change, but rather how to implement them more efficiently in code later on.\n\n\n2nd Element: The Loss Function\nHow are we going to evaluate the quality of our predictions?\nClassification Error:\n\\[l_{err}(h(x),y) = \\begin{cases} 0, & \\text{if } argmax_i h_i(x) = y \\\\ 1, & \\text{otherwise} \\end{cases}\\]\nThe error is not differentiable, so it is not good for optimization.\nA better choice: Cross-Entropy Loss or Softmax\nThe idea is that we want to map our outputs into being actual probabilities\n\\[h_i(x) \\rightarrow prob[label == i]\\]\nProbability has to be positive and sum to 1. In order to ensure \\(h_i(x)\\) is positive, we can exponentiate it. In order to ensure all \\(h_i(x)'s\\) to sum to 1, we need to normalize them.\n\\[prob[label == i] = normalize(\\exp(h(x)))= \\frac{\\exp(h_i(x))}{\\sum_{j = 1}^{k} \\exp(h_j(x))} = softmax(h(x))\\]\nThis is called the softmax operation, a mapping between scalar values and a probability distribution.\nSo now we have a probability, We need some way of quantifying whether the vector of probabilities \\(softmax(h(x))\\) is good or not. We want \\(prob[label == y]\\) to be high, as large as possible, so the loss function idea can be minimizing the negative of this probability (double negative makes a positive!).\n\\[l_{cross-entropy} (h(x), y) = - prob[label = y]\\]\nBecause minimizing probabilities is not numerically good: Probabilities are bounded between \\(0\\) and \\(1\\), so their gradients near \\(0\\) can become tiny (vanishing gradients). Logs transform that range \\((0,1)\\) into \\((−\\infty,0)\\), making the loss surface smoother and the gradients more useful. So we take the log of it\n\\[l_{ce} (h(x), y) = - log(prob[label = y])\\]\nThis is commonly known as the negative log loss or cross-entropy loss.",
    "crumbs": [
      "MACHINE LEARNING",
      "Deep Learning Systems"
    ]
  }
]