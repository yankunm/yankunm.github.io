[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Meng’s Notes",
    "section": "",
    "text": "Preface\nIf you are reading this, you may be interesed in seeing what is “Alex’s Notes”.\nThese notes are just things that I am documenting, that I wish could become a useful resource for my future students, either when I TA or become a professor.\nHere’s how to learn anything:\n\nWrite it out! (Document, Code along)\nEXPERIMENT and explore\nVisualize things you don’t understand\nAsk Questions\nAnswer Exercise and Problems (stretch your knowledge)\nShare with like-minded individuals",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "courses/dls.html",
    "href": "courses/dls.html",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "Chapter 1: Machine Learning Refresher\nIn this set of notes, we will create a minimal version of PyTorch / Tensorflow from scratch, of what I call “PyStickOnFire”.\nThe (Supervised) Machine learning idea: we take a bunch of labeled data, feed them to a machine learning algorithm, and it outputs a “program” that solves the task.\nB[Machine Learning Algorithm] B –&gt; C(Model h) ``` –&gt;\nWe will focus on what the machine learning algorithm box contains. In general, it consists of three things:\nAll alogrithms in machine learning fit in this structure. Let’s look at softmax regression to illustrate these three basic components.",
    "crumbs": [
      "Deep Learning Systems"
    ]
  },
  {
    "objectID": "courses/dls.html#chapter-1-machine-learning-refresher",
    "href": "courses/dls.html#chapter-1-machine-learning-refresher",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "The hypothesis class (the structure of \\(h\\) in terms of a set of parameters)\nThe loss function (specifies how good a given hypothesis is)\nAn optimization method (the way to minimize the loss function)\n\n\n\nMulti-class Classification (Softmax Regression)\nConsider a k-class classification setting, where we have\n\ntraining data: \\[x^{(i)} \\in \\mathbb{R}^{n}, y^{(i)} \\in \\{1, \\dots, k \\} \\text{ for } i = 1, \\dots, m\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\n\nwhere the training data are vectors that looks like \\[X = \\{ \\begin{bmatrix} x^{(1)}_1 \\\\ x^{(1)}_2 \\\\ \\vdots \\\\ x^{(1)}_n \\end{bmatrix} , \\dots, \\begin{bmatrix} x^{(m)}_1 \\\\ x^{(m)}_2 \\\\ \\vdots \\\\ x^{(m)}_n \\end{bmatrix} \\}\\] and the labels are just a set of scalars of size \\(k\\).\n\n\n1st Element: The Hypothesis Function\nThe hypothesis function is a mapping from one input to one output. (Duhh… just like every other function there is). \\[h: \\mathbb{R^n} \\rightarrow \\mathbb{R^k}\\] \\[h(x) = \\begin{bmatrix} h_1(x) \\\\ h_2(x) \\\\ \\vdots \\\\ h_k(x)\\end{bmatrix}\\]\nSo what really is \\(h_i(x)\\)? It is the hypothesis, the “belief”, the probability of how likely \\(x\\) maps to class \\(i\\).\nA linear hypothesis function uses matrix multiplication, or some other linear way, for this transformation:\n\\[h_\\theta(x) = \\theta^T x\\]\nfor parameters \\(\\theta \\in \\mathbb{R^{n \\times k}}\\) (\\(n\\) rows and \\(k\\) columns, so transpose becomes \\(k \\times n\\), and \\(x \\in \\mathbb{R^{n \\times 1}}\\), so multiplication will work). Now we say \\(h_\\theta\\) because \\(\\theta\\) is the parameters.\nNotice how so far we only have one input and one output, \\(h\\) is only working on one instance of the training set. However, in order to implement these operations efficiently in the future, we shall use the matrix batch notation.\n\\[X \\in \\mathbb{R^{m \\times n}} = \\begin{bmatrix} -x^{(1)^{T}}- \\\\ -x^{(2)^{T}}- \\\\ \\vdots \\\\ -x^{(m)^{T}}- \\end{bmatrix}\\]\n\\[y \\in \\{ 1, \\dots, k \\}^m = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}  \\end{bmatrix}\\]\n\\[h_\\theta(X) = \\begin{bmatrix} -h_\\theta(x^{(1)})^{T}- \\\\ -h_\\theta(x^{(2)})^{T}- \\\\ \\vdots \\\\ -h_\\theta(x^{(m)})^{T}- \\end{bmatrix} = \\begin{bmatrix} -x^{(1)^{T}} \\theta- \\\\ -x^{(2)^{T}} \\theta- \\\\ \\vdots \\\\ -x^{(m)^{T}} \\theta- \\end{bmatrix} = X\\theta\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\nEach row is for a data point, the first example is in first row (originally a column vector, now we transposed it to row vector), and the second example is in second row, and so on… Note that this is not merely a notation change, but rather how to implement them more efficiently in code later on.\n\n\n2nd Element: The Loss Function\nHow are we going to evaluate the quality of our predictions?\nClassification Error\n\\[l_{err}(h(x),y) = \\begin{cases} 0, & \\text{if } argmax_i h_i(x) = y \\\\ 1, & \\text{otherwise} \\end{cases}\\]\nThe error is not differentiable, so it is not good for optimization.\nA better choice: Cross-Entropy Loss or Softmax\nThe idea is that we want to map our outputs into being actual probabilities\n\\[h_i(x) \\rightarrow prob[label == i]\\]\nProbability has to be positive and sum to 1. In order to ensure \\(h_i(x)\\) is positive, we can exponentiate it. In order to ensure all \\(h_i(x)'s\\) to sum to 1, we need to normalize them.\n\\[prob[label == i] = normalize(\\exp(h(x)))= \\frac{\\exp(h_i(x))}{\\sum_{j = 1}^{k} \\exp(h_j(x))} = softmax(h(x))\\]\nThis is called the softmax operation, a mapping between scalar values and a probability distribution.\nSo now we have a probability, We need some way of quantifying whether the vector of probabilities \\(softmax(h(x))\\) is good or not. We want \\(prob[label == y]\\) to be high, as large as possible, so the loss function idea can be minimizing the negative of this probability (double negative makes a positive!).\n\\[l_{cross-entropy} (h(x), y) = - prob[label = y]\\]\nBecause minimizing probabilities is not numerically good: Probabilities are bounded between \\(0\\) and \\(1\\), so their gradients near \\(0\\) can become tiny (vanishing gradients). Logs transform that range \\((0,1)\\) into \\((−\\infty,0)\\), making the loss surface smoother and the gradients more useful. So we take the log of it\n\\[l_{ce} (h(x), y) = - log(prob[label = y]) = -h_y(x) + log \\sum_{j=1}^{k} exp(h_j(x))\\]\nThis is commonly known as the negative log loss or cross-entropy loss. This is also a case of convex optimization.\n\n\n3rd Element: Optimization\nHow do we find good values for \\(\\theta\\)?\nThis element we will spend the most time to cover because we not only what to know what the optimization is, but how we are optimizing it.\nThe following problem is the problem that almost all machine algorithms are solving. Here is the problem,\n\\[\n\\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} l(h_\\theta(x^{(i)}), y^{(i)})\n\\]\nWe are searching over all possible values of \\(\\theta\\), the one that minimizes the average loss. For example, here’s softmax regression,\n\\[\n\\min_{\\theta} f(\\theta) = \\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} l_{ce}(\\theta^T x^{(i)}, y^{(i)})\n\\]\nNow we know the “what”, but, how do we find that? How do we solve \\(\\min_{\\theta} f(\\theta) ?\\)\n\nThe Gradient\nFor our \\(f(\\theta)\\) (the function that we are trying to minimize), remember, this function takes the parameter (which is of dimension n examples by k output classes) and outputs a loss scalar, in other words \\[f: \\mathbb{R^{n \\times k}} \\rightarrow \\mathbb{R}\\]\n\\[f(\\theta) \\in \\mathbb{R}\\]\nRemember that the gradient is a multidimensional derivative that has a direction, which points to the direction of sharpest increase (locally). The gradient operator can only act on a scalar and return a vector.\nHere is the definition of the gradient in our case,\n\\[\\nabla_\\theta f(\\theta) = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\theta_{11}} \\frac{\\partial f}{\\partial \\theta_{12}} \\dots \\frac{\\partial f}{\\partial \\theta_{1k}} \\\\\n\\frac{\\partial f}{\\partial \\theta_{21}} \\frac{\\partial f}{\\partial \\theta_{22}}  \\dots \\frac{\\partial f}{\\partial \\theta_{2k}} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial \\theta_{n1}} \\frac{\\partial f}{\\partial \\theta_{n2}}  \\dots \\frac{\\partial f}{\\partial \\theta_{nk}}\n\\end{bmatrix} \\in \\mathbb{R^{n \\times k}}\\]\nThe derivative of a function is the slope of the function, change in \\(y\\) over change in \\(x\\).\n\n\nGradient Descent\nIf the gradient points in the direction of maximum increase, to minimize a function, we can repeatedly step in the opposite direction. In other words,\n\\[\\theta = \\theta - \\alpha \\nabla_\\theta f(\\theta)\\]\nwhere \\(\\alpha\\) is called the learning rate or step size. Note that the learning rate must be positive for the stepping direction to be in the negative direction from the gradient. This basic idea powers all deep learning. It is really hard to encapsulate how impactful this one line of math has been.\nThe choice of the step size \\(\\alpha\\) is really really really important. Too small slows down progress, but too big will overshoot.\n\n\nStochastic Gradient Descent\nWe split up the dataset into minibatches, which are subsets of data of size \\(B\\).\nWe repeat the process of sampling minibatches and taking steps to update \\(\\theta\\).\n\nSample: \\(X \\in \\mathbb{R^{B \\times n}}, y \\in \\{ 1, \\dots, k\\}^B\\)\nUpdate: \\(\\theta = \\theta - \\frac{\\alpha}{B} \\sum_{i=1}^{B} \\nabla_\\theta l(h_\\theta(x^{(i)}), y^{(i)})\\)\n\n\n\nCalculating the gradient in practice\nIn order to calculate the gradient of \\(f(\\theta)\\), which is essentially the sum of gradients,\n\\[\\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^{m} l(h_\\theta(x^{(i)}), y^{(i)}) =  \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta l(h_\\theta(x^{(i)}), y^{(i)})\\]\nWe have to calculate the gradient \\(m\\) times, which is very expensive. Can we reduce the number of times we take gradient? Yes, and that’s what we actually do in practice.\nAs an example, how do we compute the gradient for softmax objective? We can do it by hand, but it is cumbersome. You can use something like chain rule. We want derivative of a vector with respect to a matrix……We need some more general and generic way to take derivatives. What can we do?\nIn practice, we just specify the hypothesis function and the loss function, and use Automatic Differentiation. But how?\nWe can either do it through the “right” way, use matrix differential calculus, jacobians, kronecker products, and vectorization. Or we could take a shortcut (what everyone actually does): We pretend everything is scalar, use typical chain rule, and then transpose/rearrange outputs to make sizes work, and then check your answers numerically.\n\\[\\frac{\\partial}{\\partial \\theta} l_{ce} (\\theta^T x, y)= \\frac{\\partial l_{ce} (\\theta^T x, y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^T x}{\\partial \\theta}\\]",
    "crumbs": [
      "Deep Learning Systems"
    ]
  },
  {
    "objectID": "courses/optics.html",
    "href": "courses/optics.html",
    "title": "Optics",
    "section": "",
    "text": "Q1: One Dimensional Wave Equation\nThe wave equation is given by\n\\[\\psi(x,t) = \\frac{3}{[10(x - vt)^2 + 1]}\\]\nShow, using brute force, that this is a solution to the one dimensional differential wave equation.\nGreat! Let’s start with what is a wave.\nDef. A classical traveling wave is a self-sustaining disturbance \\(\\psi\\) of a medium, and the disturbance \\(\\psi\\) moves through space transporting energy and momentum.\nEverything is waves.\nSound! A type of longitudinal wave, where the displacement vector points parallel to the direction of motion.\nGuitar string! A type of transverse wave, where the displacement vector points perpendicular to the direction of motion.\nA wave is not a stream of particles! Because the individual atoms stay in equilibrium, but only the disturbance advances through them. Leonardo da Vinci was one of the first person to realize waves does not transport the medium through which it travels.\nImagine disturbance \\(\\psi\\) moves in positive direction \\(x\\) with constant velocity \\(v\\).\n\\[\\psi = f(x,t)\\]\nWhat is \\(f(x,0)\\)? it is the shape (aka the profile) of \\(\\psi\\) at \\(t=0\\). For example, try visualizing \\(f(x) = e^{-ax^2}\\), you’ll see that it is a gaussian function. Setting \\(t=0\\) is taking a snapshot of the pulse as it travels by.\nIn order to understand this better, let’s ignore \\(t\\) by introducing a coordinate system \\(S^{'}\\) that travels with the pilse at the speed \\(v\\). As we move with \\(S^{'}\\), the wave looks stationary! So\n\\[\\psi = f \\left( x^{'} \\right)\\]\nwhere \\(x^{'} = x - vt\\), because after time \\(t\\) the same point on \\(\\psi\\) moved a distance of \\(vt\\).",
    "crumbs": [
      "Optics"
    ]
  },
  {
    "objectID": "courses/optics.html#q1-one-dimensional-wave-equation",
    "href": "courses/optics.html#q1-one-dimensional-wave-equation",
    "title": "Optics",
    "section": "",
    "text": "General Form of One Dimensional Wave Function\n\\[\\psi(x,t) = f(x - vt)\\]\nJean Le Rond d’Alembert was the one that brought partial differential equations to physics and formulated the differential wave equation.",
    "crumbs": [
      "Optics"
    ]
  },
  {
    "objectID": "courses/quant.html",
    "href": "courses/quant.html",
    "title": "Quant",
    "section": "",
    "text": "1 Problem Simplification\nBeing a quant is knowing how to solve problems with logic, math, and intuition. These Problems come from A practical guide to Quantitative Finance Interviews by Xinfeng Zhou.",
    "crumbs": [
      "Quant"
    ]
  },
  {
    "objectID": "courses/quant.html#problem-simplification",
    "href": "courses/quant.html#problem-simplification",
    "title": "Quant",
    "section": "",
    "text": "Screwy Pirates\nFive pirates looted a chest full of 100 gold coins. Being a bunch of democratic pirates, they agree on the following method to divide the loot:\nThe most senior pirate will propose a distribution of the coins. All pirates, including the most senior pirate, will then vote. If at least \\(50\\%\\) of the pirates (\\(3\\) pirates in this case) accept the proposal, the gold is divided as proposed. If not, the most senior pirate will be fed to shark and the process starts over with the next most senior pirate… The process is repeated until a plan is approved. You can assume that all pirates are perfectly rational: they want to stay alive first and to get as much gold as possible second. Finally, being blood-thirsty pirates, they want to have fewer pirates on the boat if given a choice between otherwise equal outcomes.\nHow will the gold coins be divided in the end?\n\n\nAnswer\n\nI have no idea what the five pirates will do, lemme consider a simpler case, 1 pirate.\n1 pirate. Pirate 1 propose to distribute all 100 gold coins to himself, and accept the proposal.\n\\[100 \\text{ coins to pirate 1}\\]\n2 pirates. Pirate 2 proposes to get all the gold, 50% good, gets all the gold.\n\\[100 \\text{ coins to pirate 2}\\]\n3 pirates. From the perspective of pirate 1, pirate 1 gets nothing if pirate 3 pirate 3 gets voted out (back to case 2), so pirate 1 will try to make pirate 3 win, iff pirate 1 gets at least some benefits. Pirate 3 knows that, so pirate 3 will give 1 coin to pirate 1 and 99 coins to pirate 3, since pirate 1 will think anything is better than nothing.\n\\[1 \\text{ coin to pirate 1}, 99 \\text{ coins to pirate 3}\\]\n4 pirates. If pirate 4 gives to pirate 2, pirate 2 will vote for pirate 4 because if he doesn’t, it will be back to 3 pirates case where he doesn’t get anything…So\n\\[1 \\text{ coin pirate 2}, 99 \\text{ coins pirate 4}\\]\n5 pirates. Pirate 5 will give coinds to pirate 1 and pirate 3, because doing that will allow them to get some coins, where if he gets voted out, in 4 pirates case they don’t get anything…So\n\\[\\boxed{1 \\text{ coin for pirate 1}, 1 \\text{ coin for pirate 3}, 98 \\text{ coins for pirate 5}}\\]\nWe can actually formualte a generalizable law from this using mathematics.",
    "crumbs": [
      "Quant"
    ]
  }
]