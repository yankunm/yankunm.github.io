[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Meng’s Notes",
    "section": "",
    "text": "Preface\nIf you are reading this, you may be interesed in seeing what is “Alex’s Notes”.\nThese notes are just things that I am documenting, that I wish could become a useful resource for my future students, either when I TA or become a professor.\nHere’s how to learn anything:\n\nWrite it out! (Document, Code along)\nEXPERIMENT and explore\nVisualize things you don’t understand\nAsk Questions\nAnswer Exercise and Problems (stretch your knowledge)\nShare with like-minded individuals",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "courses/dls.html",
    "href": "courses/dls.html",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "Chapter 1: Machine Learning Refresher\nIn this set of notes, we will create a minimal version of PyTorch / Tensorflow from scratch, of what I call “PyStickOnFire”.\nThe (Supervised) Machine learning idea: we take a bunch of labeled data, feed them to a machine learning algorithm, and it outputs a “program” that solves the task.\nflowchart LR\n  A[Training Data Set X] --&gt;B[Machine Learning Algorithm]\n  B --&gt; C(Model h)\nIn this set of notes, we will focus on what the machine learning algorithm box contains. In general, it consists of three things:\nAll alogrithms in machine learning fit in this structure. Let’s look at softmax regression to illustrate these three basic components.",
    "crumbs": [
      "MACHINE LEARNING",
      "Deep Learning Systems"
    ]
  },
  {
    "objectID": "courses/dls.html#chapter-1-machine-learning-refresher",
    "href": "courses/dls.html#chapter-1-machine-learning-refresher",
    "title": "Deep Learning Systems",
    "section": "",
    "text": "The hypothesis class (the structure of \\(h\\) in terms of a set of parameters)\nThe loss function (specifies how good a given hypothesis is)\nAn optimization method (the way to minimize the loss function)\n\n\n\nMulti-class Classification (Softmax Regression)\nConsider a k-class classification setting, where we have\n\ntraining data: \\[x^{(i)} \\in \\mathbb{R}^{n}, y^{(i)} \\in \\{1, \\dots, k \\} \\text{ for } i = 1, \\dots, m\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\n\nwhere the training data are vectors that looks like \\[X = \\{ \\begin{bmatrix} x^{(1)}_1 \\\\ x^{(1)}_2 \\\\ \\vdots \\\\ x^{(1)}_n \\end{bmatrix} , \\dots, \\begin{bmatrix} x^{(m)}_1 \\\\ x^{(m)}_2 \\\\ \\vdots \\\\ x^{(m)}_n \\end{bmatrix} \\}\\] and the labels are just a set of scalars of size \\(k\\).\n\n\n1st Element: The Hypothesis Function\nThe hypothesis function is a mapping from one input to one output. (Duhh… just like every other function there is). \\[h: \\mathbb{R^n} \\rightarrow \\mathbb{R^k}\\] \\[h(x) = \\begin{bmatrix} h_1(x) \\\\ h_2(x) \\\\ \\vdots \\\\ h_k(x)\\end{bmatrix}\\]\nSo what really is \\(h_i(x)\\)? It is the hypothesis, the “belief”, the probability of how likely \\(x\\) maps to class \\(i\\).\nA linear hypothesis function uses matrix multiplication, or some other linear way, for this transformation:\n\\[h_\\theta(x) = \\theta^T x\\]\nfor parameters \\(\\theta \\in \\mathbb{R^{n \\times k}}\\) (\\(n\\) rows and \\(k\\) columns, so transpose becomes \\(k \\times n\\), and \\(x \\in \\mathbb{R^{n \\times 1}}\\), so multiplication will work). Now we say \\(h_\\theta\\) because \\(\\theta\\) is the parameters.\nNotice how so far we only have one input and one output, \\(h\\) is only working on one instance of the training set. However, in order to implement these operations efficiently in the future, we shall use the matrix batch notation.\n\\[X \\in \\mathbb{R^{m \\times n}} = \\begin{bmatrix} -x^{(1)^{T}}- \\\\ -x^{(2)^{T}}- \\\\ \\vdots \\\\ -x^{(m)^{T}}- \\end{bmatrix}\\]\n\\[y \\in \\{ 1, \\dots, k \\}^m = \\begin{bmatrix} y^{(1)} \\\\ y^{(2)} \\\\ \\vdots \\\\ y^{(m)}  \\end{bmatrix}\\]\n\\[h_\\theta(X) = \\begin{bmatrix} -h_\\theta(x^{(1)})^{T}- \\\\ -h_\\theta(x^{(2)})^{T}- \\\\ \\vdots \\\\ -h_\\theta(x^{(m)})^{T}- \\end{bmatrix} = \\begin{bmatrix} -x^{(1)^{T}} \\theta- \\\\ -x^{(2)^{T}} \\theta- \\\\ \\vdots \\\\ -x^{(m)^{T}} \\theta- \\end{bmatrix} = X\\theta\\]\n\n\\(n =\\) dimensionality of input data\n\\(k =\\) number of different classes / labels\n\\(m =\\) number of data points in the training data\n\nEach row is for a data point, the first example is in first row (originally a column vector, now we transposed it to row vector), and the second example is in second row, and so on… Note that this is not merely a notation change, but rather how to implement them more efficiently in code later on.\n\n\n2nd Element: The Loss Function\nHow are we going to evaluate the quality of our predictions?\nClassification Error:\n\\[l_{err}(h(x),y) = \\begin{cases} 0, & \\text{if } argmax_i h_i(x) = y \\\\ 1, & \\text{otherwise} \\end{cases}\\]\nThe error is not differentiable, so it is not good for optimization.\nA better choice: Cross-Entropy Loss or Softmax\nThe idea is that we want to map our outputs into being actual probabilities\n\\[h_i(x) \\rightarrow prob[label == i]\\]\nProbability has to be positive and sum to 1. In order to ensure \\(h_i(x)\\) is positive, we can exponentiate it. In order to ensure all \\(h_i(x)'s\\) to sum to 1, we need to normalize them.\n\\[prob[label == i] = normalize(\\exp(h(x)))= \\frac{\\exp(h_i(x))}{\\sum_{j = 1}^{k} \\exp(h_j(x))} = softmax(h(x))\\]\nThis is called the softmax operation, a mapping between scalar values and a probability distribution.\nSo now we have a probability, We need some way of quantifying whether the vector of probabilities \\(softmax(h(x))\\) is good or not. We want \\(prob[label == y]\\) to be high, as large as possible, so the loss function idea can be minimizing the negative of this probability (double negative makes a positive!).\n\\[l_{cross-entropy} (h(x), y) = - prob[label = y]\\]\nBecause minimizing probabilities is not numerically good: Probabilities are bounded between \\(0\\) and \\(1\\), so their gradients near \\(0\\) can become tiny (vanishing gradients). Logs transform that range \\((0,1)\\) into \\((−\\infty,0)\\), making the loss surface smoother and the gradients more useful. So we take the log of it\n\\[l_{ce} (h(x), y) = - log(prob[label = y]) = -h_y(x) + log \\sum_{j=1}^{k} exp(h_j(x))\\]\nThis is commonly known as the negative log loss or cross-entropy loss. This is also a case of convex optimization.\n\n\n3rd Element: Optimization\nHow do we find good values for \\(\\theta\\)?\nThis element we will spend the most time to cover because we not only what to know what the optimization is, but how we are optimizing it.\nThe following problem is the problem that almost all machine algorithms are solving. Here is the problem,\n\\[\n\\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} l(h_\\theta(x^{(i)}), y^{(i)})\n\\]\nWe are searching over all possible values of \\(\\theta\\), the one that minimizes the average loss. For example, here’s softmax regression,\n\\[\n\\min_{\\theta} f(\\theta) = \\min_{\\theta} \\frac{1}{m} \\sum_{i=1}^{m} l_{ce}(\\theta^T x^{(i)}, y^{(i)})\n\\]\nNow we know the “what”, but, how do we find that? How do we solve \\(\\min_{\\theta} f(\\theta) ?\\)\n\nThe Gradient\nFor our \\(f(\\theta)\\) (the function that we are trying to minimize), remember, this function takes the parameter (which is of dimension n examples by k output classes) and outputs a loss scalar, in other words \\[f: \\mathbb{R^{n \\times k}} \\rightarrow \\mathbb{R}\\]\n\\[f(\\theta) \\in \\mathbb{R}\\]\nRemember that the gradient is a multidimensional derivative that has a direction, which points to the direction of sharpest increase (locally). The gradient operator can only act on a scalar and return a vector.\nHere is the definition of the gradient in our case,\n\\[\\nabla_\\theta f(\\theta) = \\begin{bmatrix}\n\\frac{\\partial f}{\\partial \\theta_{11}} \\frac{\\partial f}{\\partial \\theta_{12}} \\dots \\frac{\\partial f}{\\partial \\theta_{1k}} \\\\\n\\frac{\\partial f}{\\partial \\theta_{21}} \\frac{\\partial f}{\\partial \\theta_{22}}  \\dots \\frac{\\partial f}{\\partial \\theta_{2k}} \\\\\n\\vdots \\\\\n\\frac{\\partial f}{\\partial \\theta_{n1}} \\frac{\\partial f}{\\partial \\theta_{n2}}  \\dots \\frac{\\partial f}{\\partial \\theta_{nk}}\n\\end{bmatrix} \\in \\mathbb{R^{n \\times k}}\\]\nThe derivative of a function is the slope of the function, change in \\(y\\) over change in \\(x\\).\n\n\nGradient Descent\nIf the gradient points in the direction of maximum increase, to minimize a function, we can repeatedly step in the opposite direction. In other words,\n\\[\\theta = \\theta - \\alpha \\nabla_\\theta f(\\theta)\\]\nwhere \\(\\alpha\\) is called the learning rate or step size. Note that the learning rate must be positive for the stepping direction to be in the negative direction from the gradient. This basic idea powers all deep learning. It is really hard to encapsulate how impactful this one line of math has been.\nThe choice of the step size \\(\\alpha\\) is really really really important. Too small slows down progress, but too big will overshoot.\nNow, another problem, calculating the gradient many many times over all data points is computationally intensive and memory expensive.\nIn order to calculate the gradient of \\(f(\\theta)\\), which is essentially the sum of gradients,\n\\[\\nabla_\\theta \\frac{1}{m} \\sum_{i=1}^{m} l(h_\\theta(x^{(i)}), y^{(i)}) =  \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta l(h_\\theta(x^{(i)}), y^{(i)})\\]\nWe have to calculate the gradient \\(m\\) times, which is very expensive. Can we reduce the number of times we take gradient? Yes, and that’s what we actually do in practice.\n\n\nStochastic Gradient Descent\nWe split up the dataset into minibatches, which are subsets of data of size \\(B\\).\nWe repeat the process of sampling minibatches and taking steps to update \\(\\theta\\).\n\nSample: \\(X \\in \\mathbb{R^{B \\times n}}, y \\in \\{ 1, \\dots, k\\}^B\\)\nUpdate: \\(\\theta = \\theta - \\frac{\\alpha}{B} \\sum_{i=1}^{B} \\nabla_\\theta l(h_\\theta(x^{(i)}), y^{(i)})\\)\n\n\n\nCalculating the gradient in practice\nAs an example, how do we compute the gradient for softmax objective? We can do it by hand, but it is cumbersome. You can use something like chain rule. We want derivative of a vector with respect to a matrix……We need some more general and generic way to take derivatives. What can we do?\nIn practice, we just specify the hypothesis function and the loss function, and use Automatic Differentiation. But how?\nWe can either do it through the “right” way, use matrix differential calculus, jacobians, kronecker products, and vectorization. Or we could take a shortcut (what everyone actually does): We pretend everything is scalar, use typical chain rule, and then transpose/rearrange outputs to make sizes work, and then check your answers numerically.\n\\[\\frac{\\partial}{\\partial \\theta} l_{ce} (\\theta^T x, y)= \\frac{\\partial l_{ce} (\\theta^T x, y)}{\\partial \\theta^T x} \\cdot \\frac{\\partial \\theta^T x}{\\partial \\theta}\\]",
    "crumbs": [
      "MACHINE LEARNING",
      "Deep Learning Systems"
    ]
  }
]